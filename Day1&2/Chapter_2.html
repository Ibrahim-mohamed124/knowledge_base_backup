<!--
title: Backup, Restore, and Migration.
description: 
published: true
date: 2025-11-07T13:33:34.735Z
tags: 
editor: ckeditor
dateCreated: 2025-11-07T08:07:57.224Z
-->

<h1>Chapter&nbsp;2.&nbsp; Backup, Restore, and Migration of Applications with OADP</h1>
<ul>
  <li>To restore an application fully in k8s the following objects must be backed up:<ul>
      <li>Kubernetes <mark class="marker-yellow">resources</mark> that define the application and its settings.</li>
      <li><mark class="marker-yellow">Container images </mark>in the internal registry that containers of this application use.</li>
      <li><mark class="marker-yellow">Data</mark> that is stored in persistent volumes or object storage for a stateful application.</li>
    </ul>
  </li>
</ul>
<h3>Backing Up Application Resources</h3>
<ul>
  <li>Backing up an entire application involves:<ul>
      <li>Listing all the application resources</li>
      <li>Exporting the application resources</li>
      <li>Cleaning the application resources</li>
      <li>Deploying the cleaned resources</li>
    </ul>
  </li>
</ul>
<blockquote>
  <p>yq utility is very useful to clean unwanted attributes from a yaml file</p>
</blockquote>
<h3>Backing Up Container Images</h3>
<ul>
  <li>Podman and Skopeo</li>
  <li>To access the registry from outside the cluster or to export images from your local machine, an OpenShift administrator must expose the registry externally.</li>
</ul>
<pre><code class="language-plaintext">oc patch \
  configs.imageregistry.operator.openshift.io/cluster \
  --patch '{"spec":{"defaultRoute":true}}' \
  --type merge</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>The modification to the image registry operator triggers a redeployment of the OpenShift API server. It can take up to 10 minutes for the cluster to stabilize.</p>
</blockquote>
<ul>
  <li>The operator creates a <code>default-route</code> route to expose externally the registry that uses the following URL format:</li>
</ul>
<pre><code class="language-plaintext">default-route-openshift-image-registry.apps-domain</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext"> REGISTRY=$(oc get \
  route default-route \
  -n openshift-image-registry \
  --template '{{.spec.host}}')</code></pre>
<p>&nbsp;</p>
<ul>
  <li>OpenShift users who do not have access to the <code>openshift-image-registry</code> namespace can retrieve the registry URL from any image stream:</li>
</ul>
<pre><code class="language-plaintext">oc -n openshift get is/cli \
  -ojsonpath="{.status.publicDockerImageRepository}{'\n'}"
</code></pre>
<ul>
  <li>You can then log in to the internal registry by using your OpenShift username and authentication token with the following command:</li>
</ul>
<pre><code class="language-plaintext">podman login \
  -u $(oc whoami) \
  -p $(oc whoami -t) \
  --tls-verify=false \
  $REGISTRY</code></pre>
<p>&nbsp;</p>
<ul>
  <li>The <code>oc registry login</code> command automatically uses your authentication token and the internal registry URL from the OpenShift cluster.</li>
  <li>The <code>oc</code> command stores the credentials in the <code>${HOME}/.docker/config.json</code> file in Base64 format. The Podman, Skopeo, and Docker clients can use the authentication details from that file to access an image registry.</li>
</ul>
<pre><code class="language-plaintext">oc registry login</code></pre>
<h4>Export Container Images</h4>
<pre><code class="language-plaintext">skopeo copy \
  docker://${REGISTRY}/project_name/imagestream:tag \ 1
  docker://remote-registry.example.com/path/to/image:remotetag 2
  
1. Fully qualified source image in the OpenShift internal registry &gt;&gt; exposed externally to export
images from outside the cluster

2. Destination registry URL with image and tag information</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>Skopeo can copy images to other locations such as a local directory or a <code>.tar</code> archive.</p>
</blockquote>
<p>or</p>
<pre><code class="language-plaintext">podman pull ${REGISTRY}/project_name/imagestream:tag

podman save ${REGISTRY}/project_name/imagestream:tag \
  | bzip2 &gt; image_backup.tar.bz2</code></pre>
<p>or</p>
<pre><code class="language-plaintext">oc image mirror ${REGISTRY}/project_name/imagestream:* \ 1
  remote-registry.example.com/path/to/image
  
1. You can use the wildcard character (*) to copy all the tags to the destination registry.

</code></pre>
<p>&nbsp;</p>
<ul>
  <li>To export a container image from within the cluster, you can use any available container tools in a pod to copy the image to the location of your choice, such as a persistent volume on NFS storage, S3 storage, or a remote registry.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: batch/v1
kind: Job
metadata:
  name: backup-image
  namespace: application
  labels:
    app: backup
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: backup
    spec:
      containers:
      - name: backup
        image: image-registry.openshift-image-registry.svc:5000/openshift/cli:latest
        env:
        - name: REGISTRY_AUTH_FILE
          value: /tmp/dockercfg.json
        command: ["/bin/bash", "-c"]
        args:
        - | 1
          oc registry login
          oc image mirror \
            image-registry.openshift-image-registry.svc:5000/application/myapp:* \
            file://myapp --dir /backup
        volumeMounts: 2
          - mountPath: /backup
            name: backup
      ...output omitted...
      volumes: 3
      - name: backup
        persistentVolumeClaim:
          claimName: backup-volume
          
1. Log in to the internal registry and export all tags of the myapp image stream to the /backup path.

2. Volume mount definition for the backup location.

3. Volume definition for the backup persistent volume claims (PVC).</code></pre>
<p>&nbsp;</p>
<ul>
  <li>You need the <code>system:image-puller</code> role on the OpenShift project to pull images from any image streams in that project. Project users and administrators already have this permission, as well as the <code>default</code> service account.</li>
</ul>
<h4>Import Container Images</h4>
<pre><code class="language-plaintext">skopeo copy \
  docker://remote-registry.example.com/myimage:latest \
  docker://${REGISTRY}/project_name/mynewimagestream:latest</code></pre>
<p>&nbsp;</p>
<p>or</p>
<pre><code class="language-plaintext"> oc image mirror \
  remote-registry.example.com/myimage:latest \
  ${REGISTRY}/project_name/mynewimagestream:latest</code></pre>
<p>&nbsp;</p>
<p>or</p>
<pre><code class="language-plaintext">podman load -i image_backup.tar.bz2

podman tag \
  registry.apps.ocp4.example.com/application/myapp:1.2.3 \
  ${REGISTRY}/newproject/myapp:1.2.3
  
podman push ${REGISTRY}/newproject/myapp:1.2.3</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>You need the <code>system:image-pusher</code> role on the OpenShift project to push images to any image streams in that project. Project users and administrators already have this permission, as well as the <code>builder</code> service account.</p>
</blockquote>
<h3>Backing Up Application Data</h3>
<ul>
  <li>Different consistency levels can be achieved depending on the backup method.<ul>
      <li><strong>Inconsistent backup</strong>
        <ul>
          <li>A backup is called inconsistent when the<mark class="marker-yellow"> application alters data during the backup process</mark>. Traditional data copying when the application is running creates inconsistent backups.</li>
        </ul>
      </li>
      <li><strong>Crash-consistent backup</strong>
        <ul>
          <li>A crash-consistent backup is created by suspending disk I/O during the backup, either by using snapshot technology or specialized tools, to ensure data consistency on disk. <mark class="marker-yellow">Application data in memory or pending I/O operations are not captured.</mark> The state of the application is kept as if the application was suddenly shut down due to power loss, or crashed.</li>
        </ul>
      </li>
      <li><strong>Application-consistent backup</strong>
        <ul>
          <li>Application-consistent backup is the most reliable type of backup because it <mark class="marker-yellow">ensures that all in-memory data and pending I/O operations are written on disk before creating the backup</mark>.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Some applications provide a set of tools to flush memory data to disk and to pause file system operations on demand. These tools provide an application-consistent backup without any downtime by using snapshots, and is also known as<mark class="marker-yellow"> </mark><i><mark class="marker-yellow">hot backup</mark></i><mark class="marker-yellow">.</mark></li>
  <li>A more universal way to create application-consistent backup is to stop the application, copy the data to another location, and then restart the application. This method is also called<mark class="marker-yellow"> </mark><i><mark class="marker-yellow">cold backup</mark></i><mark class="marker-yellow">,</mark> because the application is down during the operation.</li>
  <li>These pervious methods can halt the application for long time, so a better approach is to use volume snapshots. CSI takes a snapshot of certain volume and the backup can be created from the cloned volume without any effect on the application performance.</li>
</ul>
<h4>Volume Snapshot</h4>
<ul>
  <li>Container Storage Interface (CSI) drivers that support volume snapshots:</li>
</ul>
<figure class="table">
  <table style="background-color:rgb(255, 255, 255);">
    <thead>
      <tr>
        <th style="padding:5px;vertical-align:top;"><strong>Storage provider</strong></th>
        <th style="padding:5px;vertical-align:top;"><strong>CSI driver</strong></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="padding:5px;vertical-align:top;">AWS Elastic Block Storage</td>
        <td style="padding:5px;vertical-align:top;"><code>efs.csi.aws.com</code></td>
      </tr>
      <tr>
        <td style="padding:5px;vertical-align:top;">Azure Disk</td>
        <td style="padding:5px;vertical-align:top;"><code>disk.csi.azure.com</code></td>
      </tr>
      <tr>
        <td style="padding:5px;vertical-align:top;">CephFS</td>
        <td style="padding:5px;vertical-align:top;"><code>cephfs.csi.ceph.com</code></td>
      </tr>
      <tr>
        <td style="padding:5px;vertical-align:top;">Ceph RBD</td>
        <td style="padding:5px;vertical-align:top;"><code>rbd.csi.ceph.com</code></td>
      </tr>
      <tr>
        <td style="padding:5px;vertical-align:top;">NetApp</td>
        <td style="padding:5px;vertical-align:top;"><code>csi.trident.netapp.i</code></td>
      </tr>
    </tbody>
  </table>
</figure>
<ul>
  <li><strong>VolumeSnapshotClass:</strong>
    <ul>
      <li>a volume snapshot class describes the CSI driver and associated settings to create a volume snapshot.</li>
      <li>The <code>VolumeSnapshotClass</code> driver must match the <code>StorageClass</code> provisioner of the source PVC.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">oc get volumesnapshotclasses

apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: csi-hostpath-snapclass
driver: hostpath.csi.k8s.io
deletionPolicy: Delete
parameters:</code></pre>
<p>&nbsp;</p>
<ul>
  <li><strong>VolumeSnapshot:</strong></li>
</ul>
<pre><code class="language-plaintext">oc get volumesnapshot

apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshot
metadata:
  name: my-snapshot 1
  namespace: application 2
spec:
  volumeSnapshotClassName: ocs-storagecluster-rbdplugin-snapclass 3
  source:
    persistentVolumeClaimName: application-data 4
    
1. Name of the volume snapshot.

2. Namespace of the volume snapshot. It must be the same as the source PVC.

3. Snapshot class name for the volume snapshot.

4. Name of the source PVC that is used for the snapshot.</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>When creating an application-consistent backup, the application must be quiesced or scaled down before the snapshot creation. The application can safely be resumed or scaled up after the snapshot is created and ready to use.</p>
</blockquote>
<ul>
  <li><strong>VolumeSnapshotContent:</strong>
    <ul>
      <li>a <code>VolumeSnapshotContent</code> resource represents a snapshot that a <code>VolumeSnapshot</code> resource created.</li>
      <li>After the snapshot is created, it can function as a source to create a PVC and for a pod to use it.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-snapshot-volume 1
  namespace: application 2
spec:
  storageClassName: ocs-external-storagecluster-ceph-rbd 3
  accessModes:
  - ReadWriteOnce
  dataSource:
    apiGroup: snapshot.storage.k8s.io
    kind: VolumeSnapshot
    name: my-snapshot 4
  resources:
    requests:
      storage: 1Gi 5
      
1. Name of the PVC.

2. Namespace of the PVC. It must be the same as the snapshot namespace.

3. Storage class name for the PVC.

4. Name of the snapshot.

5. Size of the new volume. Must be equal to or greater than the snapshot size.

</code></pre>
<h4>Export Application Data</h4>
<pre><code class="language-plaintext">apiVersion: batch/v1
kind: Job
metadata:
  name: backup
  namespace: application
  labels:
    app: backup
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: backup
    spec:
      containers:
      - name: backup
        image: docker.io/d3fk/s3cmd:latest
        command: ["/bin/sh", "-c"]
        args:
        - | 1
          tar czf -C /snapshot /tmp/mybackup.tar.gz .
          s3cmd cp /tmp/mybackup.tar.gz s3://backup/
        volumeMounts: 2
          - mountPath: /snapshot
            name: snapshot
      ...output omitted...
      volumes: 3
      - name: snapshot
        persistentVolumeClaim:
          claimName: my-snapshot-volume
1. Archive the snapshot content and copy the archive to a remote S3 bucket.

2. Volume mount definition for the snapshot data in the container.

3. Volume definition for the snapshot PVC.</code></pre>
<p>&nbsp;</p>
<ul>
  <li>To export the snapshot content locally from a pod by using the <code>oc cp</code> command.</li>
</ul>
<pre><code class="language-plaintext">apiVersion: v1
kind: Pod
metadata:
  name: export
spec:
  containers:
  - image: registry.access.redhat.com/ubi9:latest
    command: ["/bin/bash", "-c"]
    args:
    - sleep infinity 1
    ...output omitted...
    volumeMounts: 2
      - mountPath: /snapshot
        name: snapshot
  volumes: 3
  - name: snapshot
    persistentVolumeClaim:
      claimName: my-snapshot-volume
1. The sleep infinity command ensures that the pod stays alive during the manual export.

2. Volume mount definition for the snapshot data in the container.

3. Volume definition for the snapshot PVC.</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext"> oc cp export:/snapshot /tmp/backup</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>The <code>tar</code> binary must be installed in the remote container for the <code>oc cp</code> command to work.</p>
</blockquote>
<h4>Import Application Data</h4>
<pre><code class="language-plaintext">apiVersion: batch/v1
kind: Job
metadata:
  name: restore
  namespace: application
  labels:
    app: restore
spec:
  backoffLimit: 1
  template:
    metadata:
      labels:
        app: restore
    spec:
      containers:
      - name: restore
        image: docker.io/d3fk/s3cmd:latest
        command: ["/bin/bash", "-c"]
        args: 1
        - |
          s3cmd cp s3://backup/mybackup.tar.gz /tmp/
          tar xvzf /tmp/mybackup.tar.gz -C /data
        volumeMounts: 2
          - mountPath: /data
            name: application-data
      ...output omitted...
      volumes: 3
      - name: application-data
        persistentVolumeClaim:
          claimName: application-data
1. Install the s3cmd tool, fetch the backup from the S3 bucket, and extract the archive inside the 
application data volume.
2. Volume mount definition for the application data in the container.

3. Volume definition for the application PVC.</code></pre>
<p>&nbsp;</p>
<h2>OADP Operator Deployment and Features</h2>
<ul>
  <li>a native backup solution for applications that run on OpenShift.</li>
  <li>It depends on open source components like:<ul>
      <li><strong>Velero: </strong>provides the backup API to the users and uses plug-ins to add extended capabilities to OADP.</li>
      <li><strong>Kopia: </strong>Velero and Data Mover use to back up persistent volumes.</li>
    </ul>
  </li>
</ul>
<figure class="image image_resized" style="width:100%;"><img src="/oadp-arch.svg"></figure>
<p>&nbsp;</p>
<ul>
  <li>the main components that OADP uses:<ol>
      <li>Velero: backs up the native kubernetes resources, and integrates with openshift plug-in to back up openshift resources and internal container images</li>
      <li>Kopia: work with CSI plug-in to move the actual data in snapshots to other destinations</li>
      <li>Data Mover: export snapshot content to object storage by using Kopia and the CSI plug-in.</li>
      <li>CSI plug-in: is used to take volume snapshots and provision volumes</li>
    </ol>
  </li>
  <li>Velero not only exports and imports resources; it also performs <mark class="marker-yellow">additional processing on the resources.&nbsp;</mark>
    <ul>
      <li>Determine the correct restore order for each resource.</li>
      <li>Apply any user-provided filters.</li>
      <li>Modify the destination namespace, if it is different from the backup.</li>
      <li>Remove or alter resource attributes such as auto-assigned node ports, IP addresses, or hostnames.</li>
      <li>Skip managed resources such as a replica set that deployment owns.</li>
    </ul>
  </li>
  <li>Data Volume Snapshot could be taken by a cloud provider snapshot APIs or by the kubernetes CSI itself. If a volume does not support either of the APIs could use the file-system backup solution FBS that is implemented by Kopia.<ul>
      <li>Kopia access the volume through the volume mount point in the cluster node so the application must be running but quiesced during the backup.</li>
    </ul>
  </li>
  <li>To take <mark class="marker-yellow">consistent backups, </mark>use the backup and restore <mark class="marker-yellow">pre-hooks</mark> to run commands before and after the backup to quiesce an application, such as a transactional database, to flush pending I/O operations to the storage back end and pause future application writing operations to avoid any data loss or corruption during the backup. A <mark class="marker-yellow">post-hook </mark>is used to resume the application operations.</li>
</ul>
<blockquote>
  <p>If you use volume snapshots, then the hooks are executed just before and after the snapshot creation, which significantly reduces the time that the application is in read-only mode.</p>
</blockquote>
<ul>
  <li>OADP Data Mover: Data Mover uses Kopia to upload and encrypt snapshot content to a unified backup repository on the object storage.&nbsp;</li>
  <li>Velero Plug-ins: extend the OADP capabilities and backup logic by using the Velero plug-in system. Support for various cloud providers, storage solutions, and additional CRDs are available with integrated plug-ins.<ul>
      <li><code><strong>openshift</strong></code>
        <ul>
          <li>Provides additional logic to back up and restore OpenShift resources, including container images from the internal registry.</li>
        </ul>
      </li>
      <li><code><strong>kubevirt</strong></code>
        <ul>
          <li>Provides additional logic to back up and restore virtual machines and other OpenShift Virtualization resources.</li>
        </ul>
      </li>
      <li><code><strong>csi</strong></code>
        <ul>
          <li>Provides volume snapshot support by using the Kubernetes CSI Snapshot API.</li>
        </ul>
      </li>
      <li>third-party plug-ins:<ul>
          <li>OpenStack plug-in for object storage on Swift and volume snapshots on Cinder.</li>
          <li>HPE plug-in for volume snapshots on HPE Nimble Storage.</li>
          <li>DigitalOcean plug-in for volume snapshots on DigitalOcean Volumes Block Storage.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<h4>OADP API Resources</h4>
<ul>
  <li><code><strong>DataProtectionApplication</strong></code>
    <ul>
      <li>The <code>dataProtectionApplication</code> resource is the configuration for the OADP operator and its components.</li>
    </ul>
  </li>
</ul>
<pre><code class="language-plaintext">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: oadp-backup
  namespace: openshift-adp
spec:
  configuration:
    velero:
      defaultPlugins:
        - aws
        - openshift
        - csi
      defaultSnapshotMoveData: true
    nodeAgent:
      enable: true
      uploaderType: kopia
  backupLocations:
    - velero:
        config:
          profile: "default"
          region: "us-east-1"
        provider: aws
        default: true
        credential:
          key: cloud
          name: cloud-credentials
        objectStorage:
          bucket: my-bucket
          prefix: oadp</code></pre>
<ul>
  <li><code><strong>BackupStorageLocation</strong></code>
    <ul>
      <li>The OADP operator creates and manages a <code>backupStorageLocation</code> resource for each storage location that is defined in the <code>dataProtectionApplication</code> configuration.</li>
    </ul>
  </li>
  <li><code><strong>VolumeSnapshotLocation</strong></code>
    <ul>
      <li>The OADP operator creates and manages a <code>volumeSnapshotLocation</code> resource for each snapshot location that is defined in the <code>dataProtectionApplication</code> configuration.</li>
    </ul>
  </li>
  <li><code><strong>Backup</strong></code>
    <ul>
      <li>The <code>backup</code> resource requests the Velero server to create a backup.</li>
    </ul>
  </li>
  <li><code><strong>Restore</strong></code>
    <ul>
      <li>The <code>restore</code> resource requests the Velero server to restore a backup.</li>
    </ul>
  </li>
  <li><code><strong>Schedule</strong></code>
    <ul>
      <li>The <code>schedule</code> resource creates a backup periodically on a given schedule by using the Cron format.</li>
    </ul>
  </li>
  <li><code><strong>BackupRepository</strong></code>
    <ul>
      <li>The <code>BackupRepository</code> resource tracks the lifecycle of the backup repositories.</li>
    </ul>
  </li>
</ul>
<h4>Requirements for OADP</h4>
<ul>
  <li>OpenShift User Permissions: cluster-admin in the openshift-adp namespace</li>
  <li>Object Storage: OADP requires an object storage location to store the backups.</li>
  <li>CSI Snapshot: a volume snapshot class must be created to register the CSI driver to use to create snapshots. The driver name must be the same as the <code>provisioner</code> attribute of the volume storage class to back up.</li>
</ul>
<blockquote>
  <p>For storage classes that do not support snapshots, such as <code>nfs-storage</code>, you can use the file-system backup.</p>
</blockquote>
<p>&nbsp;</p>
<h4>Components Configuration</h4>
<pre><code class="language-plaintext">  configuration:
    velero:
      defaultPlugins: 1
        - aws
        - openshift
        - csi
      defaultSnapShotMoveData: true 2
    nodeAgent: 3
      enable: true 4
      uploaderType: kopia 5
      podConfig: 6
        resourceAllocations:
          limits:
            cpu: "1"
            memory: 8Gi
          requests:
            cpu: 500m
            memory: 256Mi
1. Velero plug-ins to enable. The openshift plug-in is mandatory. The csi plug-in is necessary for
 snapshots and Data Mover.
2. 
Set this parameter to true to use the default Data Mover.

3. The agent that manages volume backups.

4. 
Enable the node agent if you use either FSB or Data Mover.

5. 
You can use either Kopia or Restic as the uploader. If you use the Data Mover, then you must 

use Kopia.

6. 
Optional pod configuration such as node selector, labels, and resource allocation for 

each component.</code></pre>
<h4>Backup Storage Location</h4>
<pre><code class="language-plaintext">  backupLocations:
    - velero:
        config: 1
          profile: "default" 2
          region: "us-east-1"
          s3Url: https://s3.openshift-storage.svc
          s3ForcePathStyle: "true"
          insecureSkipTLSVerify: "true"
        provider: aws 3
        default: true 4
        credential: 5
          key: cloud
          name: cloud-credentials
        objectStorage:
          bucket: my-bucket 6
          prefix: oadp 7
          caCert: LLS0S0...LS0tLS0K 8
1. 
Storage provider-specific configuration. In this example, the configuration is for the aws provider.

2. 
The credentials profile name that is defined in the Velero secret.

3. 
The storage provider plug-in name.

4. 
Use this backup location as the default one if none is specified in the backup and restore resources.

5. 
Velero secret with the storage provider credentials.

6. 
Bucket name from the storage provider.

7. Optional subdirectory in the bucket to store backups. If not specified, then the backups are stored at the root level of the bucket.

8. 
Optional CA bundle in Base64 format to verify TLS connections to the storage provider.</code></pre>
<p>&nbsp;</p>
<blockquote>
  <p>OADP can access the S3 endpoint by using two addressing models:</p>
  <ul>
    <li>The path-style model, such as <code>https://s3.mydomain.com/<i>bucket-name</i>/myfile.txt</code></li>
    <li>The DNS-style model (also known as virtual-hosted style), such as <code>\https://<i>bucket-name</i>.s3.mydomain.com/myfile.txt</code> This method is the default.</li>
  </ul>
  <p>If the object storage solution does not support the DNS-style method, then you can configure OADP to use the path-style model instead with the <code>s3ForcePathStyle</code> option.</p>
</blockquote>
<blockquote>
  <p>ImageStream backup does not work with the <code>caCert</code> option. If the S3 endpoint certificate is not trusted, then the <code>insecureSkipTLSVerify</code> attribute is required to back up container images.</p>
</blockquote>
<ul>
  <li>The Velero secret contains the credentials to access the object storage. If the secret name is not specified in the configuration, then Velero uses the <code>cloud-credentials</code> default secret name.</li>
</ul>
<pre><code class="language-plaintext"> cat credentials-velero
[myProfile] 
aws_access_key_id=AWS_ACCESS_KEY_ID
aws_secret_access_key=AWS_SECRET_ACCESS_KEY</code></pre>
<p>&nbsp;</p>
<pre><code class="language-plaintext">oc create secret generic cloud-credentials \
  --from-file cloud=credentials-velero</code></pre>
<blockquote>
  <p>The Velero secret must be created before the <code>DataProtectionApplication</code> object, or the installation will fail.</p>
</blockquote>
<h4>Snapshot Storage Location</h4>
<ul>
  <li>The <code>snapshotLocations</code> section describes the storage configuration to use for volume snapshots when using the cloud provider native snapshot capability. For CSI snapshots, this section is not needed, because volume snapshot classes are used instead.</li>
  <li>With Data Mover and FSB, backups are stored in the defined object storage location in the <code>backupLocations</code> section.</li>
</ul>
<pre><code class="language-plaintext">  snapshotLocations:
    - name: default
      velero:
        provider: aws
        config:
          region: us-west-2 1
          profile: "default" 2
        credential: 3
          key: cloud
          name: cloud-credentials
1. AWS region to use to create the snapshots.

2. The credentials profile name that is defined in the Velero secret.

3. Optional: Velero secret with the storage provider credentials.

</code></pre>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
<p>&nbsp;</p>
